---
title: "Housing Price Prediction with Principal Component Analysis"
format: pdf
editor: visual
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

### Authors: Preston O'Connor, Anthony Yasan, Matthew Jacob, Khoa Dao, Nick Wierzbowski

### Date: 4/22/2025

## Introduction

Our model uses principal component analysis to group houses on a variety of features, including geo-spatial variables and indicators of socioeconomic status of neighborhood. The data set is a subset of an original dataset obtained from the 1990 California census. Grouping housing in this manner may allow us to pinpoint different solutions most suitable for resolving the affordable housing crisis, to which Lento et al. speaks. The packages we used include cluster which is useful for cluster analysis functions, tidyverse for general visualization and data manipulation, factoextra for clustering and PCA visualization, and FactoMineR for PCA implementation. To summarize our findings, the PCA seemed to work fairly well in explaining our data and reducing it to 2 dimensions. Significant differences in variance and other factors between clusters suggest that for different categories or groupings of houses, different strategies are needed.

```{r include=FALSE}
# data to download
# delete any installed packages before uncommenting
#install.packages(c( "tidyverse","factoextra", "FactoMineR"))

library(cluster)
library(tidyverse) #for data manipulation and visualization
library(factoextra) # for the clustering and PCA visualiztion
library(FactoMineR) # for PCA implementation
# loading the csv
data <- read.csv("housing.csv")
head(data)
# nrow(data)
```

## Data Description

The dataset used for this analysis is titled **California Housing Prices**, originally sourced from the 1990 California census and made publicly available on [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices).

#### Data Structure and Size

The dataset comprises 20,640 observations (rows) and 10 variables (columns), all in numeric form except for the `ocean_proximity` variable which is categorical. Each row represents a block group, which is the smallest geographical unit for which the U.S. Census Bureau publishes sample data.

#### Variables

Below is a summary of each variable:

-   **longitude**: Geographic coordinate, measured in degrees (negative for Western Hemisphere).

-   **latitude**: Geographic coordinate, measured in degrees (positive for Northern Hemisphere).

-   **housing_median_age**: Median age of houses in the block.

-   **total_rooms**: Total number of rooms in all houses within the block.

-   **total_bedrooms**: Total number of bedrooms in all houses within the block.

-   **population**: Total population of the block.

-   **households**: Total number of households in the block.

-   **median_income**: Median income of households within the block (scaled in tens of thousands).

-   **median_house_value**: Median house value for households within the block (target variable, in USD).

-   **ocean_proximity**: Categorical variable indicating the block’s proximity to the ocean (e.g., "INLAND", "\<1H OCEAN", "NEAR OCEAN").

#### Data Cleaning

```{r}
# get rid of any non numerical features
data_clean <- data %>%
  select(where(is.numeric))
str(data_clean)
```

### Removing Unknown rows and Outliers

```{r}
total <- sum(is.na(data_clean))
total
#cleaning the data points
data_clean <- na.omit(data_clean)

total <- sum(is.na(data_clean))
total
```

-   removed 207 rows of data from the data set

#### IQR Outlier Removal

```{r}
# Note it is fine to normalize latitude and longitude for out set up
Q1 <- apply(data_clean, 2, quantile, 0.25)
Q3 <- apply(data_clean, 2, quantile, 0.75)
IQR_vals <- Q3 - Q1

in_bounds <- apply(data_clean, 1, function(row) {
  all(row >= (Q1 - 1.5 * IQR_vals) & row <= (Q3 + 1.5 * IQR_vals))
})

data_filtered <- data_clean[in_bounds, ]



# Convert matrix result of scale() to data frame
data_scaled <- as.data.frame(scale(data_filtered))
```

#### Boxplot

```{r}
data_scaled %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value, fill = Variable)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() +
  ggtitle("Boxplots After Z-Score Normalization") +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
```

-   Here, We can see there are still some minor outliers outside the normalized data set we are implementing

## Analysis

#### Applying PCA

```{r}
pca_res<- prcomp(data_scaled, center = TRUE, scale. = TRUE)
pca_res
```

-   Form the principle components we have 9 standardized features that we extracted from the data

-   PC1 is heavily influenced by the total_rooms, total_bedrooms, population, households

-   PC2 is mostly driven by spatial features like long and lat

-   PC3 relate strongly to median_income ad median_house_value

-   PC4 is dominated by the housing_median_age

```{r}
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
pca_df <- as.data.frame(pca_result$x[, 1:4])  # Use first four principal components
summary(pca_result)

```

-   We chose PC1-PC4 because these capture over 92% of the total variance in the data

-   PC1 explains 41% and PC2 adds 21.5% giving a strong reduction in dimensional with minimal information loss

    -   by implementing the top 4 PCs ensures most of our data's structure is preserved while reducing noiseand complexity

#### Cumulative Variance

```{r}
explained_var <- pca_res$sdev^2
prop_var <- explained_var / sum(explained_var)
cum_var <- cumsum(prop_var)
cum_var[1:3]

```

-   From this, we can see we should keep the first 3 components since they explain a high level of variance (81%). this will allow us to retain the most signal and reduce the noise and dimension

```{r}
# select PC1 and PC2 for the data 
# data_pca <- as.data.frame(pca_result$x[, 1:2])
# data_pca

data_scaled <- scale(data_filtered %>% select(where(is.numeric)))

# PCA
pca_res <- prcomp(data_scaled)
pca_df <- as.data.frame(pca_res$x[, 1:3]) 

# Plot first 2 principal components
fviz_pca_ind(pca_res, label = "none", addEllipses = TRUE, title = "PCA of Customer Data")
```

-   Here we can see that Dim1 and Dim2 do a good job and together explain about 62.8% of the total variance, which is strong. The data is fairly spread out and has two visible groupings from what we can tell in the graph.This suggest that our structure is suitable for a clustering implementation. This justifies our reasoning to reduce the data to the first 4 components.

#### Determine Optimal K

```{r}
fviz_nbclust(pca_df, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2) +
  labs(title = "Elbow Method for Optimal K")
```

-   Here we see that the k=5 is the reasonable value as this is where the curve is starting to flatten out

#### Run K-means Clustering (Wondering how the test works here)

```{r}
set.seed(123)

km <- kmeans(pca_df, centers = 5, nstart = 50)


sil <- silhouette(km$cluster, dist(pca_df))
avg_sil <- mean(sil[, 3])


variance_explained <- 1 - (km$tot.withinss / km$totss)


cat("Silhouette score (k = 5):", round(avg_sil, 3), "\n")
cat("Variance explained (k = 5):", round(variance_explained, 3), "\n")
```

-   For the Silhouette score of 0.302 we are in the somewhat moderate range. Many points may be reasonable close to their cluster centroids; however, there is bound to be overlap between the clusters.

-   We captured a reasonably strong clustering that pertains to a meaningful structure

## Model Evaluation and Prediction

#### Evaluation of silhoute

```{r}
fviz_silhouette(sil)
```

-   The average silhouette widt is 0.3 showcasing moderate cluster separation

-   Cluster 1 and 4 show somewhat high silhouette widths, suggesting better internal cohesion.

-   Clusters 2 and 5 have lower and more variable silhouettes scores, implying overlap and poor separations

-   Overall, while some of the clusters are indeed well-formed, others may benefit from re-evaluation or a different choice of k

#### Variance and Silhouette

```{r}
set.seed(123)

k_values <- 2:10
silhouette_scores <- numeric(length(k_values))
variance_scores <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  km <- kmeans(pca_df, centers = k_values[i], nstart = 50)
  sil <- silhouette(km$cluster, dist(pca_df))
  
  silhouette_scores[i] <- mean(sil[, 3])
  variance_scores[i] <- 1 - (km$tot.withinss / km$totss)
}

results <- data.frame(
  k = k_values,
  silhouette = silhouette_scores,
  variance = variance_scores
)


ggplot(results, aes(x = k)) +
  geom_line(aes(y = silhouette), color = "blue", size = 1) +
  geom_point(aes(y = silhouette), color = "blue") +
  geom_line(aes(y = variance), color = "red", size = 1) +
  geom_point(aes(y = variance), color = "red") +
  labs(title = "Silhouette (blue) vs Variance Explained (red)",
       x = "Number of Clusters (k)",
       y = "Score") +
  theme_minimal()
```

-   This shows a trade off between silhouette and variance as the number of our clusters increase. While variance explained continues to rise with more clusters, the silhouette score peaks around k=2, 3, then declines indicating that the cluster quality drops beyond that point. This suggest an optimal balance m

#### PCA Scatter Plot

```{r}
set.seed(123)
km_result <- kmeans(pca_df, centers = 5, nstart = 25)

# Then plot
fviz_cluster(km_result, data = pca_df,
             geom = "point",           # points only, no text
             ellipse.type = "norm",    # shaded cluster region
             palette = "Set2",
             show.clust.cent = TRUE,   # show centroids
             ggtheme = theme_minimal(),
             main = "K-means Clustering (PCA-reduced)")
```

-   This k-means cluster plot reduced by PCA with k = 5 indicates distinct but overlapping clusters, representing moderate suggesting moderate separation in the data. The two principal components (Dim1 and Dim2) explain around 66.6% of the variance, allowing one to meaningfully 2d-visualize the structure. While clusters 1 and 2 appear quite compact, other such cluster 5 overlap considerably more, indicating potential ambiguity in those group boundaries

## Conclusion and Summary

SA6090292 Conclusion In this project, we applied Principal Component Analysis (PCA) and K-means clustering to look at housing data from the 1990 California census into groups based upon various features including geographical location, socioeconomic indicators, and housing characteristics. After preprocessing the data by removing outliers and handling NA values, we successfully improved the quality of the dataset. By then extracting the first four principal components, we were able to capture over 92% of the total variance in the data, simplifying the complex dataset. Using our processed data, we implemented K-means clustering, determining that five clusters best fit the data via the elbow method. The clustering results showed moderate silhouette scores, indicating some grouping of the data but also areas of overlap between clusters. The clusters revealed areas for potential improvement in cluster separation, especially in clusters 2 and 5 where the silhouette scores were lower. The findings suggest that while PCA and K-means are somewhat useful tools for grouping houses based on key factors, the clustering model would benefit from further refinement. Additional consideration may be needed in choosing the optimal number of clusters. Further developments could involve other clustering techniques or could incorporate more relevant variables in order to improve model capability. Nonetheless, this analysis offers some insight into the segmentation of California housing, insight that could lead to more targeted strategies for addressing the California housing crisis.

## References

-   Nugent, C. (2017, November 24). California housing prices. Kaggle. https://www.kaggle.com/datasets/camnugent/california-housing-prices?select=housing.csv

-   Kassambara. (2018, October 21). K-means clustering in R: Algorithm and practical examples. Datanovia. https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/

-   Kryńska, K. (n.d.). Using K-means and PAM clustering for Customer Segmentation. RPubs. https://rpubs.com/kkrynska/USL_k-means \>\>\>\>\>\>\> 02ae82986c97859750f8e7d4ad40bae2c4c0faf6

-   Lento, Rochelle E., Shaun Donovan, Sheila Crowley, Rebecca L. Peace, Mark H. Shelburne, Jeanne Peterson, Janet Kennedy, et al. “The Future of Affordable Housing.” Journal of Affordable Housing & Community Development Law 20, no. 2 (2011): 215–50. http://www.jstor.org/stable/41429170.
