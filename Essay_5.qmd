---
title: "Math_456_essay_5"
format: pdf
editor: visual
---

# Title

### Authors: Preston O'Connor

### Date: 4/22/2025

## Introduction

```{r}
# data to download
# delete any installed packages before uncommenting
#install.packages(c( "tidyverse","factoextra", "FactoMineR"))

library(cluster)
library(tidyverse) #for data manipulation and visualization
library(factoextra) # for the clustering and PCA visualiztion
library(FactoMineR) # for PCA implementation
# loading the csv
data <- read.csv("housing.csv")
head(data)
# nrow(data)
```

## Data Description

#### Data Cleaning

```{r}
# get rid of any non numerical features
data_clean <- data %>%
  select(where(is.numeric))


str(data_clean)
# check for missing data points

```

### Removing Unknown rows and Outliers

```{r}
total <- sum(is.na(data_clean))
total

data_clean <- na.omit(data_clean)


total <- sum(is.na(data_clean))
total
```

-   removed any rows with unknwon empty categories

#### IQR Outlier Removal

```{r}
# Note it is fine to normalize latitude and longitude for out set up
Q1 <- apply(data_clean, 2, quantile, 0.25)
Q3 <- apply(data_clean, 2, quantile, 0.75)
IQR_vals <- Q3 - Q1

in_bounds <- apply(data_clean, 1, function(row) {
  all(row >= (Q1 - 1.5 * IQR_vals) & row <= (Q3 + 1.5 * IQR_vals))
})

data_filtered <- data_clean[in_bounds, ]



# Convert matrix result of scale() to data frame
data_scaled <- as.data.frame(scale(data_filtered))
```

#### Boxplot

```{r}
data_scaled %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value, fill = Variable)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() +
  ggtitle("Boxplots After Z-Score Normalization") +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
```

-   Here, the Income column has a single outlier. We will implement a short and simple IQR set up to remove the outlier, with its corresponding row, from the data_clean table

## Analysis

#### Applying PCA

```{r}
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
pca_result
```

#### Cumulative Variance

```{r}
explained_var <- pca_res$sdev^2
prop_var <- explained_var / sum(explained_var)

# Cumulative variance for first 4 components
cum_var <- cumsum(prop_var)

# Display cumulative variance for PC1â€“PC4
cum_var[1:3]
```

-   From this, we can see we should keep the first 3 components since they explain a high level of variance. this will allow us to retain the most signal and reduce the noise and dimension

```{r}
# select PC1 and PC2 for the data 
# data_pca <- as.data.frame(pca_result$x[, 1:2])
# data_pca

data_scaled <- scale(data_filtered %>% select(where(is.numeric)))

# PCA
pca_res <- prcomp(data_scaled)
pca_df <- as.data.frame(pca_res$x[, 1:3]) 

# Plot first 2 principal components
fviz_pca_ind(pca_res, label = "none", addEllipses = TRUE, title = "PCA of Customer Data")
```

-   Here we can see that Dim1 and Dim2 do a good job and together explain about 72.8% of the total variance, which is strong. The data is fairly spread out and has two visible groupings from what we can tell in the graph.

#### Determine the optimal K

```{r}

fviz_nbclust(pca_df, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2) +
  labs(title = "Elbow Method for Optimal K")
```

#### Run K-means Clustering (Wondering how the test works here)

```{r}
set.seed(123)

# Ensure clean PCA with 4 components only
data_scaled <- scale(data_filtered %>% select(where(is.numeric)))
pca_res <- prcomp(data_scaled)
pca_df <- as.data.frame(pca_res$x[, 1:4])  # Only PC1 to PC4

# 10-fold setup
n <- nrow(pca_df)
folds <- cut(seq(1, n), breaks = 10, labels = FALSE)

cv_results <- data.frame(Fold = integer(), Silhouette = double(), ExplainedVar = double())

for (i in 1:10) {
  # Split data
  test_index <- which(folds == i)
  train_data <- pca_df[-test_index, ]
  test_data <- pca_df[test_index, ]  # not used here, but could be

  # Train K-means with k = 4
  km_model <- kmeans(train_data, centers = 4, nstart = 25)

  # Silhouette score on training set
  sil <- silhouette(km_model$cluster, dist(train_data))
  silhouette_avg <- mean(sil[, 3])

  # Explained variance
  explained_var <- km_model$betweenss / km_model$totss

  # Store
  cv_results <- rbind(cv_results, data.frame(Fold = i, Silhouette = silhouette_avg, ExplainedVar = explained_var))
}

# View results
print(cv_results)

# Plot silhouette over folds
ggplot(cv_results, aes(x = Fold, y = Silhouette)) +
  geom_line() + geom_point(size = 2) +
  labs(title = "Silhouette Score Across 10-Fold CV (k = 4)", y = "Silhouette Score", x = "Fold") +
  theme_minimal()
```

#### Cluster Visualization

```{r}
# explained_variance <- km_result$betweenss / km_result$totss
# cat("Explained Variance Ratio:", round(explained_variance, 3), "\n")
# 
# sil <- silhouette(km_result$cluster, dist(data_pca))
# cat("Average Silhouette Score:", round(mean(sil[, 3]), 3), "\n")



```

## Model Evaluation and Prediction

#### Evaluation of siloute

```{r}
fviz_silhouette(sil)
```

#### PCA Scatter Plot

```{r}
set.seed(123)
km_result <- kmeans(pca_df, centers = 4, nstart = 25)

# Then plot
fviz_cluster(km_result, data = pca_df,
             geom = "point",           # points only, no text
             ellipse.type = "norm",    # shaded cluster region
             palette = "Set2",
             show.clust.cent = TRUE,   # show centroids
             ggtheme = theme_minimal(),
             main = "K-means Clustering (PCA-reduced)")
```

## Conclusion and Summary

## References

-   <https://www.kaggle.com/datasets/camnugent/california-housing-prices?select=housing.csv>

-   2

-   3
