---
title: "Math_456_essay_5"
format: pdf
editor: visual
---

# Title

### Authors: Preston O'Connor

### Date: 4/22/2025

## Introduction

```{r}
# data to download
# delete any installed packages before uncommenting
#install.packages(c( "tidyverse","factoextra", "FactoMineR"))

library(cluster)
library(tidyverse) #for data manipulation and visualization
library(factoextra) # for the clustering and PCA visualiztion
library(FactoMineR) # for PCA implementation
# loading the csv
data <- read.csv("housing.csv")
head(data)
# nrow(data)
```

## Data Description

## Data Description

The dataset used for this analysis is titled **California Housing Prices**, originally sourced from the 1990 California census and made publicly available on [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices).

#### Data Structure and Size

The dataset comprises 20,640 observations (rows) and 10 variables (columns), all in numeric form except for the `ocean_proximity` variable which is categorical. Each row represents a block group, which is the smallest geographical unit for which the U.S. Census Bureau publishes sample data.

#### Variables

Below is a summary of each variable:

-   **longitude**: Geographic coordinate, measured in degrees (negative for Western Hemisphere).

-   **latitude**: Geographic coordinate, measured in degrees (positive for Northern Hemisphere).

-   **housing_median_age**: Median age of houses in the block.

-   **total_rooms**: Total number of rooms in all houses within the block.

-   **total_bedrooms**: Total number of bedrooms in all houses within the block.

-   **population**: Total population of the block.

-   **households**: Total number of households in the block.

-   **median_income**: Median income of households within the block (scaled in tens of thousands).

-   **median_house_value**: Median house value for households within the block (target variable, in USD).

-   **ocean_proximity**: Categorical variable indicating the block’s proximity to the ocean (e.g., "INLAND", "\<1H OCEAN", "NEAR OCEAN").

#### Data Cleaning

```{r}
# get rid of any non numerical features
data_clean <- data %>%
  select(where(is.numeric))
str(data_clean)
```

### Removing Unknown rows and Outliers

```{r}
total <- sum(is.na(data_clean))
total
#cleaning the data points
data_clean <- na.omit(data_clean)

total <- sum(is.na(data_clean))
total
```

-   removed 207 rows of data from the data set

#### IQR Outlier Removal

```{r}
# Note it is fine to normalize latitude and longitude for out set up
Q1 <- apply(data_clean, 2, quantile, 0.25)
Q3 <- apply(data_clean, 2, quantile, 0.75)
IQR_vals <- Q3 - Q1

in_bounds <- apply(data_clean, 1, function(row) {
  all(row >= (Q1 - 1.5 * IQR_vals) & row <= (Q3 + 1.5 * IQR_vals))
})

data_filtered <- data_clean[in_bounds, ]



# Convert matrix result of scale() to data frame
data_scaled <- as.data.frame(scale(data_filtered))
```

#### Boxplot

```{r}
data_scaled %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value, fill = Variable)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() +
  ggtitle("Boxplots After Z-Score Normalization") +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
```

-   Here, We can see there are still some minor outliers outside the normalized data set we are implementing

## Analysis

#### Applying PCA

```{r}
pca_res<- prcomp(data_scaled, center = TRUE, scale. = TRUE)
pca_res
```

-   Form the principle components we have 9 standardized features that we extracted from the data

-   PC1 is heavily influenced by the total_rooms, total_bedrooms, population, households

-   PC2 is mostly driven by spatial features like long and lat

-   PC3 relate strongly to median_income ad median_house_value

-   PC4 is dominated by the housing_median_age

```{r}
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
pca_df <- as.data.frame(pca_result$x[, 1:4])  # Use first four principal components
summary(pca_result)

```

-   We chose PC1-PC4 because these capture over 92% of the total variance in the data

-   PC! explains 41% and PC2 adds 21.5% giving a strong reduction in dimensional with minimal information loss

    -   by implementing the top 4 PCs ensures most of our data's structure is preserved while reducing noiseand complexity

#### Cumulative Variance

```{r}
explained_var <- pca_res$sdev^2
prop_var <- explained_var / sum(explained_var)
cum_var <- cumsum(prop_var)
cum_var[1:3]

```

-   From this, we can see we should keep the first 3 components since they explain a high level of variance (81%). this will allow us to retain the most signal and reduce the noise and dimension

```{r}
# select PC1 and PC2 for the data 
# data_pca <- as.data.frame(pca_result$x[, 1:2])
# data_pca

data_scaled <- scale(data_filtered %>% select(where(is.numeric)))

# PCA
pca_res <- prcomp(data_scaled)
pca_df <- as.data.frame(pca_res$x[, 1:3]) 

# Plot first 2 principal components
fviz_pca_ind(pca_res, label = "none", addEllipses = TRUE, title = "PCA of Customer Data")
```

-   Here we can see that Dim1 and Dim2 do a good job and together explain about 62.8% of the total variance, which is strong. The data is fairly spread out and has two visible groupings from what we can tell in the graph.This suggest that our structure is suitable for a clustering implementation. This justifies our reasoning to reduce the data to the first 4 components.

#### Determine Optimal K

```{r}
fviz_nbclust(pca_df, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2) +
  labs(title = "Elbow Method for Optimal K")
```

-   Here we see that the k=5 is the reasonable value as this is where the curve is starting to flatten out

#### Run K-means Clustering (Wondering how the test works here)

```{r}
set.seed(123)

km <- kmeans(pca_df, centers = 5, nstart = 50)


sil <- silhouette(km$cluster, dist(pca_df))
avg_sil <- mean(sil[, 3])


variance_explained <- 1 - (km$tot.withinss / km$totss)


cat("Silhouette score (k = 5):", round(avg_sil, 3), "\n")
cat("Variance explained (k = 5):", round(variance_explained, 3), "\n")
```

-   For the Silhouette score of 0.302 we are in the somewhat moderate range. Many points may be reasonable close to their cluster centroids; however, there is bound to be overlap between the clusters.

-   We captured a reasonably strong clustering that pertains to a meaningful structure

## Model Evaluation and Prediction

#### Evaluation of silhoute

```{r}
fviz_silhouette(sil)
```

-   The average silhouette widt is 0.3 showcasing moderate cluster separation

-   Cluster 1 and 4 show somewhat high silhouette widths, suggesting better internal cohesion.

-   Clusters 2 and 5 have lower and more variable silhouettes scores, implying overlap and poor separations

-   Overall, while some of the clusters are indeed well-formed, others may benefit from re-evaluation or a different choice of k

#### Variance and Silhouette

```{r}
ggplot(results, aes(x = k)) +
  geom_line(aes(y = silhouette), color = "blue", size = 1) +
  geom_point(aes(y = silhouette), color = "blue") +
  geom_line(aes(y = variance), color = "red", size = 1) +
  geom_point(aes(y = variance), color = "red") +
  labs(title = "Silhouette (blue) vs Variance Explained (red)",
       x = "Number of Clusters (k)",
       y = "Score") +
  theme_minimal()
```

-   This shows a trade off between silhouette and variance as the number of our clusters increase. While variance explained continues to rise with more clusters, the silhouette score peaks around k=2, 3, then declines indicating that the cluster quality drops beyond that point. This suggest an optimal balance m

#### PCA Scatter Plot

```{r}
set.seed(123)
km_result <- kmeans(pca_df, centers = 5, nstart = 25)

# Then plot
fviz_cluster(km_result, data = pca_df,
             geom = "point",           # points only, no text
             ellipse.type = "norm",    # shaded cluster region
             palette = "Set2",
             show.clust.cent = TRUE,   # show centroids
             ggtheme = theme_minimal(),
             main = "K-means Clustering (PCA-reduced)")
```

-   This k-means cluster plot reduced by PCA with k = 5 indicates distinct but overlapping clusters, representing moderate suggesting moderate separation in the data. The two principal components (Dim1 and Dim2) explain around 66.6% of the variance, allowing one to meaningfully 2d-visualize the structure. While clusters 1 and 2 appear quite compact, other such cluster 5 overlap considerably more, indicating potential ambiguity in those group boundaries

## Conclusion and Summary

## References

-   <https://www.kaggle.com/datasets/camnugent/california-housing-prices?select=housing.csv>

-   2

-   3
